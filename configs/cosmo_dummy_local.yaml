output_dir: results/cosmo-dummy-test

data:
    name: dummy
    n_train: 4
    n_valid: 4
    sample_shape: [128, 128, 128, 4]
    target_shape: [4]
    batch_size: 1
    n_epochs: 8
    shard: True

model:
    name: cosmoflow
    input_shape: [128, 128, 128, 4]
    target_size: 4
    n_conv_layers: 1
    conv_size: 2
    fc1_size: 16
    fc2_size: 8
    hidden_activation: LeakyReLU
    pooling_type: MaxPool3D
    dropout: 0.5

optimizer:
    name: SGD
    momentum: 0.9

lr_schedule:
    # Standard linear LR scaling configuration, tested up to batch size 1024
    base_lr: 0.001
    scaling: linear
    base_batch_size: 64

    # Alternate sqrt LR scaling which has worked well for batch size 512-1024.
    #base_lr: 0.0025
    #scaling: sqrt
    #base_batch_size: 32

    n_warmup_epochs: 2

    # You may want to adjust these decay epochs depending on your batch size.
    # E.g. if training batch size 64 you may want to decay at 16 and 32 epochs.
    decay_schedule:
        4: 0.25
        6: 0.125

train:
    loss: mse
    metrics: ['mae']
    target_mae: 0.124
