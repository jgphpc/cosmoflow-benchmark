{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict     \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%env MLPERF_HPC_ROOT=/home/lukasd/src/mlperf\n",
    "\n",
    "#%env MLPERF_COSMO_DATA_BENCHMARK_TIMESTAMP=data_benchmark/2020-08-05_23-48-09_daint101/\n",
    "#%env MLPERF_COSMO_DATA_BENCHMARK_TIMESTAMP=data_benchmark/2020-08-05_00-21-54_daint101/\n",
    "\n",
    "%env MLPERF_COSMO_DATA_BENCHMARK_TIMESTAMP=data_benchmark/2020-08-06_15-25-54_daint101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"## Data benchmark experiments ##\"\n",
    "set -x\n",
    "tree ${MLPERF_HPC_ROOT}/cosmoflow-benchmark/results/${MLPERF_COSMO_DATA_BENCHMARK_TIMESTAMP}\n",
    "set +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(result_dir):\n",
    "    config_file = os.path.join(result_dir, 'config.pkl')\n",
    "    with open(config_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_result(result_dir):\n",
    "    history_file = os.path.join(result_dir, 'data_benchmark_history.csv')\n",
    "    history = pd.read_csv(history_file)\n",
    "    for col in ['local_times', 'global_times']:\n",
    "        history[col] = history[col].apply(lambda x: np.fromstring(x.strip('[]'), dtype=float, sep=' '))\n",
    "    return history\n",
    "\n",
    "def get_num_samples(config, ranks):\n",
    "    dconf = config['data']\n",
    "    n = dconf['n_train'] + dconf['n_valid']\n",
    "    if not dconf['shard']:\n",
    "        n *= ranks\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from a single data benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.expandvars('${MLPERF_HPC_ROOT}/cosmoflow-benchmark/results/${MLPERF_COSMO_DATA_BENCHMARK_TIMESTAMP}/gpu-n%i-inter2-intra12') % 2\n",
    "load_config(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_result(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# The following functions filter config records for relevant entries \n",
    "# (rewrite them to analyze performance over different config variables)\n",
    "def get_rank_samples(ranks, config):\n",
    "    return OrderedDict(ranks=ranks,\n",
    "                       samples=get_num_samples(config, ranks))    \n",
    "\n",
    "\n",
    "def get_rank_samples_device_props(ranks, config):\n",
    "    return OrderedDict(ranks=ranks,\n",
    "                       samples=get_num_samples(config, ranks),\n",
    "                       **config['device'])    \n",
    "\n",
    "\n",
    "# compute per-run performance metrics on selection of epochs\n",
    "def compute_epoch_selection_summary(config_param, benchmark_history):\n",
    "    if isinstance(benchmark_history, pd.Series):\n",
    "        local_times = benchmark_history['local_times']\n",
    "        global_times = benchmark_history['global_times']\n",
    "    else:\n",
    "        local_times = np.vstack(benchmark_history['local_times'].to_numpy())\n",
    "        global_times = np.vstack(benchmark_history['global_times'].to_numpy())\n",
    "\n",
    "    n_ranks =  config_param['ranks']\n",
    "    n_samples = config_param['samples']\n",
    "    \n",
    "    local_throughputs = n_samples / (n_ranks*local_times)\n",
    "    global_throughputs = n_samples / global_times\n",
    "\n",
    "    return dict(total_time_mean=np.mean(global_times),\n",
    "                total_time_std=np.std(global_times),\n",
    "                local_throughput_mean=np.mean(local_throughputs),\n",
    "                local_throughput_std=np.std(local_throughputs),\n",
    "                total_throughput_mean=np.mean(global_throughputs),\n",
    "                total_throughput_std=np.std(global_throughputs))\n",
    "\n",
    "# compute per-run performance metrics\n",
    "def compute_data_benchmark_summary(config_param, benchmark_history):\n",
    "\n",
    "    initial_epoch_stats = compute_epoch_selection_summary(config_param, \n",
    "                                                          benchmark_history.loc[0])\n",
    "    if benchmark_history.shape[0] > 1:\n",
    "        later_epoch_stats = compute_epoch_selection_summary(config_param,\n",
    "                                                            benchmark_history[benchmark_history['epoch'] > 0])\n",
    "    else:\n",
    "        later_epoch_stats = {}\n",
    "    return initial_epoch_stats, later_epoch_stats\n",
    "    \n",
    "    \n",
    "# Compute ideal vs. effective weak scaling relative to single node\n",
    "def compute_scaling(stats):\n",
    "    stats['local_ideal'] = stats.ranks.apply(lambda r: 1.) * stats['local_throughput_mean'].loc[0]\n",
    "    stats['local_eff']   = stats.apply(lambda result: result['local_throughput_mean']/result['local_ideal'], axis=1)\n",
    "\n",
    "    stats['total_ideal'] = stats.ranks * stats['total_throughput_mean'].loc[0]\n",
    "    stats['total_eff']   = stats.apply(lambda result: result['total_throughput_mean']/result['total_ideal'], axis=1)    \n",
    "\n",
    "    return stats\n",
    "    \n",
    "\n",
    "\n",
    "def get_experimental_results(paths, ranks, extract_config_params, extract_results, compute_extra_metrics=None):\n",
    "    \"\"\"\n",
    "    Loops over ranks-paths-pairs and computes throughput metrics.\n",
    "    Configuration parameters and results to keep can be specified with `extract_config_params`, `extract_results`.\n",
    "    Extra metrics on first and higher epochs can be added to DataFrames in `compute_extra_metrics`.\n",
    "    Returns results in a dataframe.\n",
    "    \"\"\"    \n",
    "    configs, results = [], []\n",
    "    for (path,r) in zip(paths,ranks):\n",
    "        result_dir = path\n",
    "        configs.append(load_config(result_dir))\n",
    "        results.append(load_result(result_dir))\n",
    "\n",
    "    config_params = [extract_config_params(r,c) for (r,c) in zip(ranks, configs)]\n",
    "\n",
    "    for config in config_params:\n",
    "        if config.keys() != config_params[0].keys():\n",
    "            raise RuntimeError('Extracted config keys must be the same for all experiments.')\n",
    "    config_keys = list(config_params[0].keys())\n",
    "            \n",
    "    init_summaries, later_summaries = \\\n",
    "        zip(*[compute_data_benchmark_summary(conf,res) for (conf,res) in zip(config_params, results)])\n",
    "\n",
    "    results = [extract_results(r) for r in results]\n",
    "    \n",
    "    # Merge config and summary (result for double checking)\n",
    "    init_stats  = pd.DataFrame.from_records([{**conf, **res, **summ} for (conf, res, summ) in zip(config_params, results, init_summaries )])\n",
    "    later_stats = pd.DataFrame.from_records([{**conf, **res, **summ} for (conf, res, summ) in zip(config_params, results, later_summaries)])\n",
    "    \n",
    "    if compute_extra_metrics is not None:\n",
    "        init_stats  = compute_extra_metrics(init_stats)\n",
    "        later_stats = compute_extra_metrics(later_stats)\n",
    "    \n",
    "#     stats = init_stats.merge(later_stats, on=['ranks','samples'], suffixes=('_init','_later'))\n",
    "    stats = init_stats.merge(later_stats, on=config_keys, suffixes=('_init','_later'))\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Ignore results (except for computed throughput metrics)\n",
    "def discard_results(result):\n",
    "    return {}\n",
    "\n",
    "# Standard weak scaling metrics\n",
    "def get_scaling_results(path_pattern, ranks):\n",
    "    paths = [path_pattern % r for r in ranks]\n",
    "    return get_experimental_results(paths, ranks, get_rank_samples, discard_results, compute_scaling)\n",
    "\n",
    "# Extract device config parameters in addition to above metrics\n",
    "def get_device_prop_results(paths, ranks):\n",
    "    return get_experimental_results(paths, ranks, get_rank_samples_device_props, discard_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading benchmark\n",
    "\n",
    "Weak scaling of data loading with 256 training samples & 64 validation samples per rank (one rank per node), up to 1024 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "results = get_scaling_results(\n",
    "    os.path.expandvars('${MLPERF_HPC_ROOT}/cosmoflow-benchmark/results/${MLPERF_COSMO_DATA_BENCHMARK_TIMESTAMP}/gpu-n%i-inter2-intra12'),\n",
    "    ranks=np.array([2**i for i in range(11)]))\n",
    "results[['ranks', 'samples', 'total_time_mean_init',  'local_throughput_mean_init',  'total_throughput_mean_init',  'local_ideal_init',  'local_eff_init',  'total_ideal_init',  'total_eff_init']] \n",
    "results[['ranks', 'samples', 'total_time_mean_later', 'local_throughput_mean_later', 'total_throughput_mean_later', 'local_ideal_later', 'local_eff_later', 'total_ideal_later', 'total_eff_later']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['ranks', 'samples', 'total_time_mean_init',  'local_throughput_mean_init',  'total_throughput_mean_init',  'local_ideal_init',  'local_eff_init',  'total_ideal_init',  'total_eff_init']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['ranks', 'samples', 'total_time_mean_later', 'local_throughput_mean_later', 'total_throughput_mean_later', 'local_ideal_later', 'local_eff_later', 'total_ideal_later', 'total_eff_later']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(8,8),\n",
    "                               gridspec_kw=dict(height_ratios=[.8, .2], hspace=0))\n",
    "\n",
    "ax0.set_title('Initial data loading (first epoch)')\n",
    "ax0.plot(results.ranks, results.local_throughput_mean_init, 'o-',  ms=8, label='Per-rank throughput', color='tab:blue')\n",
    "ax0.fill_between(results.ranks, results.local_throughput_mean_init - results.local_throughput_std_init, results.local_throughput_mean_init + results.local_throughput_std_init, alpha=0.2, color='tab:blue')\n",
    "ax0.plot(results.ranks, results.total_throughput_mean_init, '^-', ms=8, label='Total throughput', color='tab:orange')\n",
    "ax0.fill_between(results.ranks, results.total_throughput_mean_init - results.total_throughput_std_init, results.total_throughput_mean_init + results.total_throughput_std_init, alpha=0.2, color='tab:orange')\n",
    "ax0.plot(results.ranks, results.local_ideal_init, '--',  label='Per-rank ideal', color='tab:blue')\n",
    "ax0.plot(results.ranks, results.total_ideal_init, '--', label='Total ideal', color='tab:orange')\n",
    "ax0.set_ylabel('Data loading throughput [samples/s]')\n",
    "ax0.set_yscale('log')\n",
    "ax0.legend(loc=0)\n",
    "ax0.grid()\n",
    "\n",
    "# Scaling efficiency\n",
    "ax1.plot(results.ranks, results.local_eff_init, 'o-', ms=8, color='tab:blue')\n",
    "ax1.plot(results.ranks, results.total_eff_init, '^-', ms=8, color='tab:orange')\n",
    "ax1.set_xlabel('Number of workers')\n",
    "ax1.set_ylabel('Efficiency')\n",
    "ax1.set_ylim(bottom=0.4)\n",
    "ax1.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks(results.ranks)\n",
    "ax1.xaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "ax1.grid()\n",
    "\n",
    "# Customize y-axis\n",
    "throughput_ticks = np.array([(1.*scale, 3.*scale) for scale in np.logspace(0,4,5)]).flatten()[1:-1] #[100, 300, 1000, 3000, 10000, 30000, 100000]\n",
    "ax0.set_yticks(throughput_ticks)\n",
    "ax0.yaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "ax1.set_yticks(np.linspace(0.5, 1., 6))\n",
    "ax1.yaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(8,8),\n",
    "                               gridspec_kw=dict(height_ratios=[.8, .2], hspace=0))\n",
    "\n",
    "ax0.set_title('Repeated data loading (later epochs)')\n",
    "ax0.plot(results.ranks, results.local_throughput_mean_later, 'o-',  ms=8, label='Per-rank throughput', color='tab:red')\n",
    "ax0.fill_between(results.ranks, results.local_throughput_mean_later - results.local_throughput_std_later, results.local_throughput_mean_later + results.local_throughput_std_later, alpha=0.2, color='tab:red')\n",
    "ax0.plot(results.ranks, results.total_throughput_mean_later, '^-', ms=8, label='Total throughput', color='tab:cyan')\n",
    "ax0.fill_between(results.ranks, results.total_throughput_mean_later - results.total_throughput_std_later, results.total_throughput_mean_later + results.total_throughput_std_later, alpha=0.2, color='tab:cyan')\n",
    "ax0.plot(results.ranks, results.local_ideal_later, '--',  label='Local ideal', color='tab:red')\n",
    "ax0.plot(results.ranks, results.total_ideal_later, '--', label='Total ideal', color='tab:cyan')\n",
    "ax0.set_ylabel('Data loading throughput [samples/s]')\n",
    "ax0.set_yscale('log')\n",
    "ax0.legend(loc=0)\n",
    "ax0.grid()\n",
    "\n",
    "# Scaling efficiency\n",
    "ax1.plot(results.ranks, results.local_eff_later, 'o-', ms=8, color='tab:red')\n",
    "ax1.plot(results.ranks, results.total_eff_later, '^-', ms=8, color='tab:cyan')\n",
    "ax1.set_xlabel('Number of workers')\n",
    "ax1.set_ylabel('Efficiency')\n",
    "ax1.set_ylim(bottom=0.4)\n",
    "ax1.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks(results.ranks)\n",
    "ax1.xaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "ax1.grid()\n",
    "\n",
    "# Customize y-axis\n",
    "throughput_ticks = np.array([(1.*scale, 3.*scale) for scale in np.logspace(2,4,3)]).flatten() #[100, 300, 1000, 3000, 10000, 30000, 100000]\n",
    "ax0.set_yticks(throughput_ticks)\n",
    "ax0.yaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "ax1.set_yticks(np.linspace(0.5, 1., 6))\n",
    "ax1.yaxis.set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "For 256 training and 64 validation samples per node and batch size 4, we see near optimal weak scaling of throughput up to 64 nodes in the first epoch, from where the efficiency decays to 50 % at 1024. The locally measured throughput (based on local epoch time) follows closely the globally measured throughput (overall epoch time) so that the performance can be associated with the distributed filesystem. This is also consistent with the reduction of the local throughput. The loss in global throughput cannot be due to the global synchronization mechanism used to define the global epoch time (then the gap local-global would be bigger) and neither other data transfer components as the later epochs show much higher throughput.\n",
    "\n",
    "For higher epochs, we observe a near optimal local epoch time as the data is now cached in memory on each node. The reduced global efficiency is due to the overhead that Horovod adds for synchronizing to define the global epoch end. This should disappear when running the benchmark with more data per node (longer epoch times).\n",
    "\n",
    "For both, the initial and the higher epochs we observe a stable duration (low standard deviation), which is reflected in the displayed throughput values.\n",
    "\n",
    "This measurement was conducted at local batch size 4, however, submission candidates for Cosmoflow often use a value of 1 or 2 for this, which may lead to different results. \n",
    "\n",
    "Comparison to weak scaling of training results (an actual SGD-step adds extra work both per-node - local gradient computation - and globally - hovorod.allreduce() of local gradients). The number of samples do not agree due to different number of validation samples chosen (64 vs. 256). Nevertheless, from the data benchmark results of the initial epoch and the weak scaling of training above (only higher epochs displayed) we can conclude that if the data set does not fit in memory the training performance of a Cosmoflow neural network will be reduced by the filesystem bottleneck by a factor of 2x and even more above ~600 samples/s (64 nodes in data benchmark, ~> 32 nodes in the training benchmark, where the dataset is held in memory). This may be both due to latency effects (low node count) and throughput limits (high node count) of filesystem operations. To fit the full 5.1 TB in memory on 64 GB RAM nodes, at least 80 GPU nodes are needed, but to also train Cosmoflow, the number of GPU nodes (memory) required might actually be significantly higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing device parameters (thread pools)\n",
    "\n",
    "In the following, we run the data benchmark on single nodes with varying values of `inter_op_parallelism_threads` and `intra_op_parallelism_threads` (see https://github.com/tensorflow/tensorflow/blob/b21bc388a142c2c15a57af59f9d57ca3413f0c07/tensorflow/core/protobuf/config.proto#L379)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env MLPERF_COSMO_DATA_BENCHMARK_DEVICE_PROP_TIMESTAMP=data_benchmark/2020-08-05_11-31-33_daint101/\n",
    "%env MLPERF_COSMO_DATA_BENCHMARK_DEVICE_PROP_TIMESTAMP=data_benchmark/2020-08-05_19-54-10_daint101/\n",
    "\n",
    "path_pattern = os.path.expandvars('${MLPERF_HPC_ROOT}/cosmoflow-benchmark/results/${MLPERF_COSMO_DATA_BENCHMARK_DEVICE_PROP_TIMESTAMP}/gpu-n%i-inter%i-intra%i')\n",
    "\n",
    "ranks_range = [1]\n",
    "inter_threads = [0, 1, 2, 3, 4]\n",
    "intra_threads = [0, 6, 12, 24, 36, 48]\n",
    "\n",
    "paths = []\n",
    "ranks = []\n",
    "\n",
    "for rank in ranks_range:\n",
    "    for inter in inter_threads:\n",
    "        for intra in intra_threads:\n",
    "            paths.append(path_pattern % (rank, inter, intra))\n",
    "            ranks.append(rank)\n",
    "\n",
    "results = get_device_prop_results(\n",
    "     paths,ranks)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D-heat maps of single-node throughput over inter_threads/intra_threads-domain\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, sharex=True, figsize=(14,7))\n",
    "\n",
    "ax0.set_title('first epoch')\n",
    "image = results[['inter_threads','intra_threads','total_throughput_mean_init']].to_numpy().reshape((len(inter_threads), len(intra_threads), 3))[:,:,2]\n",
    "pos = ax0.contourf( intra_threads, inter_threads, image,cmap='hot')\n",
    "ax0.set_ylabel('inter_op_threads')\n",
    "ax0.set_yticks(inter_threads)\n",
    "ax0.set_xlabel('intra_op_threads')\n",
    "ax0.set_xticks(intra_threads)\n",
    "ax0.grid()\n",
    "fig.colorbar(pos, ax=ax0)\n",
    "\n",
    "ax1.set_title('later epochs')\n",
    "image = results[['inter_threads','intra_threads','total_throughput_mean_later']].to_numpy().reshape((len(inter_threads), len(intra_threads), 3))[:,:,2]\n",
    "pos = ax1.contourf( intra_threads, inter_threads, image,cmap='hot')\n",
    "ax1.set_ylabel('inter_op_threads')\n",
    "ax1.set_yticks(inter_threads)\n",
    "ax1.set_xlabel('intra_op_threads')\n",
    "ax1.set_xticks(intra_threads)\n",
    "ax1.grid()\n",
    "fig.colorbar(pos, ax=ax1)\n",
    "\n",
    "fig.suptitle('single-node throughput in data loading benchmark [samples/s]')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Group-by inter_threads-/intra_threads-line plots\n",
    "def group_by_plot(ax, group_by_key, x_var, y_var, title=None, x_label=None, y_label=None):\n",
    "    for key, grp in results.groupby([group_by_key], sort=False):\n",
    "        ax.plot(grp[x_var], grp[y_var], '^-',  ms=8, label=key)\n",
    "        \n",
    "    grouped_by_x = results[[x_var,y_var]].groupby([x_var], sort=False)[y_var]\n",
    "    grouped_by_x_mean = grouped_by_x.mean()\n",
    "    grouped_by_x_std = grouped_by_x.std()\n",
    "    ax.plot(grouped_by_x_mean.reset_index()[x_var], grouped_by_x_mean, '--',  ms=8, label='mean', color='black')\n",
    "    ax.fill_between(grouped_by_x_std.reset_index()[x_var], grouped_by_x_mean - grouped_by_x_std, grouped_by_x_mean + grouped_by_x_std, alpha=0.2, color='black', label='+/-std')\n",
    "\n",
    "    \n",
    "    ax.set_xticks(grp[x_var])\n",
    "    ax.set_xlabel(x_var if x_label is None else x_label)\n",
    "    ax.set_ylabel(y_var if y_label is None else y_label)\n",
    "\n",
    "    ax.legend(title=group_by_key, loc='center right', bbox_to_anchor=(1.4, 0.5))\n",
    "    ax.grid()\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "        \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(16,4))\n",
    "\n",
    "fig.suptitle('initial data loading (first epoch)')\n",
    "\n",
    "group_by_plot(axs[0], 'intra_threads', 'inter_threads', 'total_throughput_mean_init', y_label='throughput [samples/s]')\n",
    "group_by_plot(axs[1], 'inter_threads', 'intra_threads', 'total_throughput_mean_init', y_label='throughput [samples/s]')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(16,4))\n",
    "\n",
    "fig.suptitle('repeated data loading (later epochs)')\n",
    "\n",
    "group_by_plot(axs[0], 'intra_threads', 'inter_threads', 'total_throughput_mean_later', y_label='throughput [samples/s]')\n",
    "group_by_plot(axs[1], 'inter_threads', 'intra_threads', 'total_throughput_mean_later', y_label='throughput [samples/s]')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "(CPU specification see below)\n",
    "\n",
    "- `inter_threads` (left column): Setting this to 1 (number of sockets) generally leads to worse performance than leaving it at 0. For `intra_threads` in {12, 24}, the best performance is obtained at `inter_threads` = 0, although, higher values should be tested.\n",
    "- `intra_threads` (right column): The first epoch shows that `intra_threads` should be set to no more than the max. number of running threads (24), whereas the repeated data loading measurements demonstrate that also should not be smaller than the core number (12) on the machine.\n",
    "\n",
    "The heatmaps suggest that the configurations (12,0) and (24,2) (or a mix of the two components) may yield optimal performance. This measurement was done with exclusive access to the data during for each benchmark process, but should be repeated with the actual model training. The same experiment could also be repeated on the multi-core partition if one is interested in the pure I/O-performance.\n",
    "\n",
    "This topic has also been discussed [here](https://groups.google.com/a/tensorflow.org/d/msg/discuss/UQx7G9Gs7tI/ZYDcbEPkBQAJ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
